# Project 3C: Data Warehouse & Cloud Integration

This project automates data ingestion and transformation from raw files in AWS S3 into a Snowflake data warehouse. It leverages Apache Airflow for orchestration and dbt for data transformation and testing.

---

## üìÅ Project Structure (Key Folders)

| Folder       | Purpose                                              |
|--------------|------------------------------------------------------|
|`dags/`       | Airflow DAGSs                                        |
|`data/`       | Data files (raw, processed, staging)                 |
|`dbt/`        | dbt project and models                               |
|`diagrams/`   | Architecture diagrams and lineage screenshots        |
|`docker/`     | Docker config (if applicable)                        |
|`docs/`       | Documentation and project notes                      |
|`logs/`       | Airflow and pipeline logs                            |
|`sandbox/`    | Experimental scripts / tests                         |
|`tests/`      | dbt and Airflow validation tests                     |
|`.gitignore/` | Git ignore rules                                     |
|`README.md/`  | This file                                            |


## ‚ö°Tech Stack

- **AWS S3** - Data staging and file storage
- **Snowflake** - Cloud-based data warehouse
- **dbt** - Data transformation and testing framework
- **Apache Airflow** - Orchestration and automation of data workflows
- **Python** - Supporting data processing scripts (as needed)

## üöÄ Objectives

- ‚úÖ Ingest raw data from S3 into Snowflake using native `COPY INTO` commands
- ‚úÖ Transform data in Snowflake using dbt models (`staging`, `intermediate`, `final`)
- ‚úÖ Orchestrate the entire workflow in Airflow, ensuring reliability and modularity
- ‚úÖ Practice secrets management (locally via `.env` files, with awareness of AWS Secret Manager for production)
- ‚úÖ Create a clear, modular, and professional Git repo structure
- ‚úÖ Document challenges and solutions to showcase real-world Data Engineering logic

## üîê Secrets Management

Sensitive credentials (Snowflake, AWS, dbt profiles) are stored in local `.env` files and **excluded from version control** via `.gitignore`. For production environments, integration with **AWS Secrets Manager** or **HashiCorp Vault** is recommended for enhanced security.

---

## üé® Visuals

- Architecture diagrams (`/diagrams/`)
- dbt model lineage screenshots
- Airflow DAG graph screenshots

---

## üìù Developer Notes

Document challenges, solutions, and future improvements in: `docs/developer_notes.md`

---

## üöÄ Getting Started (Local)

- 1Ô∏è‚É£ Clone this repo:

```bash
git clone https://github.com/kmdawkins/portfolio-project3c.git
cd portfolio-project3c
```

- 2Ô∏è‚É£ Create and activate a virtual environment:

```bash
python -m venv .venv
.\.venv\Scripts\Activate
```

- 3Ô∏è‚É£ Install dependencies

```bash
pip install -r requirements.txt
```

- 4Ô∏è‚É£ Customize `.env` file with local environemnt variables
